# Airbnb NYC Price Prediction with NumPy

> Dá»± Ä‘oÃ¡n giÃ¡ thuÃª Airbnb táº¡i New York City sá»­ dá»¥ng Linear Regression Ä‘Æ°á»£c implement hoÃ n toÃ n báº±ng NumPy

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org)
[![NumPy](https://img.shields.io/badge/NumPy-1.24+-orange.svg)](https://numpy.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#-giá»›i-thiá»‡u)
- [Dataset](#-dataset)
- [Method](#-method)
- [Installation & Setup](#-installation--setup)
- [Usage](#-usage)
- [Results](#-results)
- [Project Structure](#-project-structure)
- [Challenges & Solutions](#-challenges--solutions)
- [Future Improvements](#-future-improvements)
- [Contributors](#-contributors)
- [Contact](#-contact)
- [License](#-license)

---

## Giá»›i thiá»‡u

### MÃ´ táº£ bÃ i toÃ¡n

Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c **dá»± Ä‘oÃ¡n giÃ¡ thuÃª cá»§a cÃ¡c listing Airbnb táº¡i New York City** dá»±a trÃªn cÃ¡c Ä‘áº·c Ä‘iá»ƒm nhÆ° vá»‹ trÃ­ Ä‘á»‹a lÃ½, loáº¡i phÃ²ng, sá»‘ lÆ°á»£ng Ä‘Ã¡nh giÃ¡, vÃ  tÃ­nh kháº£ dá»¥ng. ÄÃ¢y lÃ  bÃ i toÃ¡n regression Ä‘iá»ƒn hÃ¬nh trong lÄ©nh vá»±c Data Science vÃ  Machine Learning.

### Äá»™ng lá»±c vÃ  á»©ng dá»¥ng thá»±c táº¿

**Táº¡i sao bÃ i toÃ¡n nÃ y quan trá»ng?**

- **Cho chá»§ nhÃ  (Hosts):** XÃ¡c Ä‘á»‹nh má»©c giÃ¡ tá»‘i Æ°u Ä‘á»ƒ cáº¡nh tranh vÃ  tá»‘i Ä‘a hÃ³a doanh thu
- **Cho khÃ¡ch thuÃª:** ÄÃ¡nh giÃ¡ xem giÃ¡ listing cÃ³ há»£p lÃ½ so vá»›i thá»‹ trÆ°á»ng
- **Cho nhÃ  Ä‘áº§u tÆ°:** PhÃ¢n tÃ­ch tiá»m nÄƒng ROI cá»§a cÃ¡c khu vá»±c khÃ¡c nhau
- **Cho Airbnb platform:** Äá» xuáº¥t giÃ¡ tá»± Ä‘á»™ng vÃ  phÃ¡t hiá»‡n pricing anomalies

**á»¨ng dá»¥ng thá»±c táº¿:**
- Dynamic pricing tools
- Market analysis dashboards  
- Investment opportunity identification
- Automated pricing recommendations

### Má»¥c tiÃªu cá»¥ thá»ƒ

1. **Implement Linear Regression tá»« Ä‘áº§u** sá»­ dá»¥ng thuáº§n NumPy (khÃ´ng dÃ¹ng scikit-learn)
2. **Ãp dá»¥ng Ä‘áº§y Ä‘á»§ quy trÃ¬nh Data Science:** EDA â†’ Preprocessing â†’ Modeling â†’ Evaluation
3. **PhÃ¢n tÃ­ch vÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i nghiá»‡p vá»¥:**
   - Host nÃ o báº­n rá»™n nháº¥t vÃ  táº¡i sao?
   - Sá»± khÃ¡c biá»‡t vá» lÆ°u lÆ°á»£ng giá»¯a cÃ¡c khu vá»±c?
   - Yáº¿u tá»‘ nÃ o áº£nh hÆ°á»Ÿng máº¡nh nháº¥t Ä‘áº¿n giÃ¡?
4. **Äáº¡t performance cháº¥p nháº­n Ä‘Æ°á»£c:** RMSE < $50, RÂ² > 0.5
5. **Cung cáº¥p insights vÃ  recommendations** cho cÃ¡c stakeholders

---

## Dataset

### Nguá»“n dá»¯ liá»‡u

- **TÃªn Dataset:** Airbnb NYC 2019
- **Nguá»“n:** [Kaggle - New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data)

### MÃ´ táº£ cÃ¡c features

Dataset gá»“m **16 features** vá»›i cÃ¡c thÃ´ng tin chi tiáº¿t:

| Feature | Kiá»ƒu dá»¯ liá»‡u | MÃ´ táº£ | VÃ­ dá»¥ |
|---------|--------------|-------|-------|
| `id` | Integer | ID duy nháº¥t cá»§a listing | 2539 |
| `name` | String | TÃªn listing | "Clean & quiet apt home by the park" |
| `host_id` | Integer | ID cá»§a host | 2787 |
| `host_name` | String | TÃªn host | "John" |
| `neighbourhood_group` | Categorical | Quáº­n (Manhattan, Brooklyn, Queens...) | "Brooklyn" |
| `neighbourhood` | Categorical | Khu vá»±c cá»¥ thá»ƒ | "Kensington" |
| `latitude` | Float | VÄ© Ä‘á»™ | 40.64749 |
| `longitude` | Float | Kinh Ä‘á»™ | -73.97237 |
| `room_type` | Categorical | Loáº¡i phÃ²ng (Entire home/apt, Private, Shared) | "Private room" |
| **`price`** | **Integer** | **GiÃ¡ thuÃª/Ä‘Ãªm (USD) - Target variable** | **149** |
| `minimum_nights` | Integer | Sá»‘ Ä‘Ãªm tá»‘i thiá»ƒu | 1 |
| `number_of_reviews` | Integer | Tá»•ng sá»‘ reviews | 9 |
| `last_review` | Date | NgÃ y review cuá»‘i cÃ¹ng | 2019-05-21 |
| `reviews_per_month` | Float | Sá»‘ reviews trung bÃ¬nh/thÃ¡ng | 0.21 |
| `calculated_host_listings_count` | Integer | Sá»‘ listing cá»§a host | 6 |
| `availability_365` | Integer | Sá»‘ ngÃ y available trong nÄƒm | 365 |

### KÃ­ch thÆ°á»›c vÃ  Ä‘áº·c Ä‘iá»ƒm dá»¯ liá»‡u

**KÃ­ch thÆ°á»›c:**
- **Sá»‘ samples:** 48,895 listings
- **Sá»‘ features:** 16 cá»™t
- **Dung lÆ°á»£ng:** ~5 MB

**Äáº·c Ä‘iá»ƒm quan trá»ng:**

1. **Missing Values:**
   - `name`: ~16 missing
   - `host_name`: ~21 missing
   - `last_review`: ~10,052 missing (20.5%)
   - `reviews_per_month`: ~10,052 missing (20.5%)

2. **PhÃ¢n phá»‘i Price:**
   - Mean: $152.72
   - Median: $106
   - Std: $240.15
   - Range: $0 - $10,000
   - **Highly skewed** vá»›i nhiá»u outliers

3. **PhÃ¢n phá»‘i Categorical:**
   - **Room Type:** Entire home/apt (52%), Private room (45.7%), Shared room (2.4%)
   - **Neighbourhood Group:** Manhattan (44.3%), Brooklyn (41.1%), Queens (9.7%), Bronx (2.3%), Staten Island (0.8%)

4. **Challenges:**
   - Outliers cá»±c lá»›n trong price ($10,000)
   - Missing values Ä‘Ã¡ng ká»ƒ trong reviews
   - Imbalanced distribution giá»¯a cÃ¡c neighbourhood groups

---

## Method

### Quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u

```
Raw Data â†’ EDA â†’ Preprocessing â†’ Feature Engineering â†’ Modeling â†’ Evaluation
```

#### **1. Exploratory Data Analysis (EDA)**

- Load data báº±ng NumPy vÃ  CSV module
- Thá»‘ng kÃª mÃ´ táº£: mean, median, std, quartiles
- PhÃ¢n tÃ­ch missing values
- Visualization: histograms, box plots, correlation matrix
- PhÃ¢n tÃ­ch theo categorical features (room_type, neighbourhood_group)

#### **2. Data Preprocessing**

**Xá»­ lÃ½ Missing Values:**
```python
# Reviews_per_month: Äiá»n 0 (listing chÆ°a cÃ³ reviews)
# CÃ¡c cá»™t sá»‘ khÃ¡c: Äiá»n median (robust vá»›i outliers)
# Price, location: Loáº¡i bá» rows (thÃ´ng tin báº¯t buá»™c)
```

**Xá»­ lÃ½ Outliers:**
```python
# Loáº¡i bá»: price > $1000 (luxury segment, khÃ´ng Ä‘áº¡i diá»‡n)
# Loáº¡i bá»: minimum_nights > 365 (cÃ³ thá»ƒ lÃ  lá»—i data)
```

**Feature Engineering:**
- One-hot encoding: `room_type` â†’ 3 binary features
- One-hot encoding: `neighbourhood_group` â†’ 5 binary features
- Feature má»›i: `review_frequency` = `number_of_reviews` / (`reviews_per_month` + 1)

**Normalization:**
- Min-Max Scaling: $(x - x_{min}) / (x_{max} - x_{min})$
- Ãp dá»¥ng cho táº¥t cáº£ features sá»‘

#### **3. Train/Test Split**

- Train: 80% (random shuffle vá»›i seed=42)
- Test: 20%
- Äáº£m báº£o reproducibility

### Thuáº­t toÃ¡n sá»­ dá»¥ng

#### **Linear Regression with Gradient Descent**

**Giáº£ thuyáº¿t (Hypothesis):**

$$h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n = \theta^T x$$

Trong Ä‘Ã³:
- $\theta$ = vector há»‡ sá»‘ (weights + bias)
- $x$ = vector features
- $n$ = sá»‘ lÆ°á»£ng features

**Loss Function (Mean Squared Error):**

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$$

Vá»›i regularization L2 (Ridge):

$$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2$$

**Gradient Descent Update Rule:**

$$\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}$$

Cá»¥ thá»ƒ:

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m}\theta_j$$

Trong Ä‘Ã³:
- $\alpha$ = learning rate (0.1)
- $\lambda$ = regularization parameter (0.01)
- $m$ = sá»‘ samples

### CÃ¡ch implement báº±ng NumPy

#### **Matrix Operations**

```python
# Forward pass (Prediction)
y_pred = X @ weights + bias  # Matrix multiplication: (m, n) @ (n, 1) = (m, 1)

# Gradient computation
dw = (1/m) * (X.T @ (y_pred - y_true))  # (n, m) @ (m, 1) = (n, 1)
db = (1/m) * np.sum(y_pred - y_true)    # Scalar

# Add L2 regularization to gradient
dw += (lambda_/m) * weights

# Update parameters
weights -= learning_rate * dw
bias -= learning_rate * db
```

#### **Key NumPy Techniques**

1. **Broadcasting:** Tá»± Ä‘á»™ng má»Ÿ rá»™ng dimensions cho phÃ©p operations giá»¯a arrays khÃ¡c size
2. **Vectorization:** Thay tháº¿ loops báº±ng array operations (nhanh hÆ¡n 10-100x)
3. **Boolean Indexing:** Filter data hiá»‡u quáº£: `data[data[:, price_col] < 1000]`
4. **Array Stacking:** Káº¿t há»£p features: `np.hstack([feature1, feature2])`

#### **Evaluation Metrics Implementation**

```python
# Mean Squared Error
MSE = np.mean((y_true - y_pred) ** 2)

# Root Mean Squared Error
RMSE = np.sqrt(MSE)

# Mean Absolute Error
MAE = np.mean(np.abs(y_true - y_pred))

# RÂ² Score
ss_res = np.sum((y_true - y_pred) ** 2)
ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
R2 = 1 - (ss_res / ss_tot)
```

#### **K-Fold Cross Validation**

```python
# Chia dá»¯ liá»‡u thÃ nh k folds
n_samples = len(X)
indices = np.arange(n_samples)
np.random.shuffle(indices)

fold_size = n_samples // k_folds
for i in range(k_folds):
    val_start = i * fold_size
    val_end = (i + 1) * fold_size if i < k_folds - 1 else n_samples
    
    val_indices = indices[val_start:val_end]
    train_indices = np.concatenate([indices[:val_start], indices[val_end:]])
    
    # Train vÃ  evaluate cho má»—i fold
```

---

## Installation & Setup

### Prerequisites

- Python 3.8 hoáº·c cao hÆ¡n
- pip package manager
- Jupyter Notebook hoáº·c JupyterLab

### ThÆ° viá»‡n yÃªu cáº§u

```
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
jupyter>=1.0.0
```

### CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t

#### 1. Clone repository

```bash
git clone https://github.com/hnaht277/CSC17104-NUMPY-FOR-DATA-SCIENCE.git
cd CSC17104-NUMPY-FOR-DATA-SCIENCE
```

#### 2. Táº¡o virtual environment (khuyáº¿n nghá»‹)

**Windows:**
```bash
python -m venv venv
venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

#### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

#### 4. Kiá»ƒm tra cÃ i Ä‘áº·t

```bash
python -c "import numpy; print(f'NumPy version: {numpy.__version__}')"
```

---

## Usage

### HÆ°á»›ng dáº«n cháº¡y tá»«ng pháº§n

Project Ä‘Æ°á»£c chia thÃ nh 3 notebooks chÃ­nh, nÃªn cháº¡y theo thá»© tá»±:

#### **Step 1: Data Exploration** 

```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

**Ná»™i dung:**
- Load vÃ  inspect dá»¯ liá»‡u
- Thá»‘ng kÃª mÃ´ táº£ chi tiáº¿t
- PhÃ¢n tÃ­ch missing values
- Visualization: histograms, box plots, bar charts
- Ma tráº­n correlation
- PhÃ¢n tÃ­ch cÃ¡c cÃ¢u há»i nghiá»‡p vá»¥

**Output:** Insights vá» dá»¯ liá»‡u vÃ  biá»ƒu Ä‘á»“ trá»±c quan

#### **Step 2: Data Preprocessing** 

```bash
jupyter notebook notebooks/02_preprocessing.ipynb
```

**Ná»™i dung:**
- Xá»­ lÃ½ missing values
- Loáº¡i bá» outliers
- Feature engineering (one-hot encoding)
- Normalization (Min-Max scaling)
- Save processed data

**Output:** `data/processed/processed_data.csv`

#### **Step 3: Model Training & Evaluation** 

```bash
jupyter notebook notebooks/03_modeling.ipynb
```

**Ná»™i dung:**
- Load processed data
- Split train/test (80/20)
- Train Linear Regression model
- Evaluate performance
- K-Fold Cross Validation
- Feature importance analysis

**Output:** Trained model vÃ  evaluation metrics

---

## Results

### Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c (Metrics)

#### **Training Performance**

| Metric | Train Set | Test Set |
|--------|-----------|----------|
| **MSE** | 1,850.24 | 1,923.47 |
| **RMSE** | $43.01 | $43.86 |
| **MAE** | $29.84 | $30.12 |
| **RÂ² Score** | 0.562 | 0.549 |

**Giáº£i thÃ­ch:**
- **RMSE ~$44:** Sai sá»‘ trung bÃ¬nh khoáº£ng $44, cháº¥p nháº­n Ä‘Æ°á»£c
- **RÂ² ~0.55:** Model giáº£i thÃ­ch Ä‘Æ°á»£c 55% variance
- **Train vs Test gáº§n nhau:** KhÃ´ng bá»‹ overfitting

### Key Insights tá»« Model

**Feature Importance:**
- **Room type Entire home/apt** cÃ³ áº£nh hÆ°á»Ÿng máº¡nh nháº¥t (+$52)
- **Manhattan location** premium cao (+$38)
- **Brooklyn** cÅ©ng cÃ³ premium Ä‘Ã¡ng ká»ƒ (+$24)

---

## Project Structure

```
CSC17104-NUMPY-FOR-DATA-SCIENCE/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ AB_NYC_2019.csv          # Dataset gá»‘c
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ processed_data.csv       # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb    # EDA vÃ  visualization
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb       # Data cleaning
â”‚   â””â”€â”€ 03_modeling.ipynb            # Training & evaluation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.py           # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ visualization.py             # Váº½ biá»ƒu Ä‘á»“
â”‚   â””â”€â”€ models.py                    # Linear Regression
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Giáº£i thÃ­ch chá»©c nÄƒng

**src/modules:**
- `data_processing.py`: Load, clean, transform data
- `visualization.py`: Plotting functions
- `models.py`: Linear Regression implementation

---

## Challenges & Solutions

### KhÃ³ khÄƒn vá»›i NumPy

**1. Mixed data types:**
```python
# Giáº£i phÃ¡p: DÃ¹ng dtype=object
data = np.array(data_list, dtype=object)
```

**2. One-hot encoding:**
```python
# Implement manually
encoded = np.zeros((len(data), len(unique_values)))
```

**3. Gradient Descent khÃ´ng converge:**
```python
# Giáº£i phÃ¡p: Feature scaling + tune learning rate
X = (X - X.min()) / (X.max() - X.min())
```

---

## Future Improvements

- [ ] Polynomial Regression
- [ ] Advanced feature engineering
- [ ] Web application deployment
- [ ] Time series analysis
- [ ] Interactive dashboard

---

## ğŸ‘¥ Contributors

**NgÃ´ Há»“ng Thanh**
- Student ID: 23127475

**Course:** CSC17104 - Data Science, HK7 (2023-2024)

---

## Contact

- ğŸ“§ Email: nhthanh23@clc.fitus.edu.vn
- ğŸ”— GitHub: [@hnaht277](https://github.com/hnaht277)
- ğŸ“¦ Repository: [CSC17104-NUMPY-FOR-DATA-SCIENCE](https://github.com/hnaht277/CSC17104-NUMPY-FOR-DATA-SCIENCE)

---

## License

This project is created for learning purposes